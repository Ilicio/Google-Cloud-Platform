{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b505de45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gera_texto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgera_texto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logaction\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gera_texto'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gera_texto import logaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770ded83",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/home/ilicio/projetos/chaves/estudos-ilicio-cbb35bf6f539.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9dbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n",
    "import json\n",
    "import re\n",
    "from google.cloud import vision\n",
    "from google.cloud import storage\n",
    "\n",
    "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 100\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.Feature(\n",
    "        type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "\n",
    "    gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.OutputConfig(\n",
    "        gcs_destination=gcs_destination, batch_size=batch_size)\n",
    "\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        features=[feature], input_config=input_config,\n",
    "        output_config=output_config)\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])\n",
    "\n",
    "    print('Waiting for the operation to finish.')\n",
    "    operation.result(timeout=420)\n",
    "    \n",
    "def write_to_text(gcs_destination_uri):\n",
    "    # Once the request has completed and the output has been\n",
    "    # written to GCS, we can list all the output files.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
    "    bucket_name = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List objects with the given prefix.\n",
    "    blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "    print('Output files:')\n",
    "\n",
    "    transcription = open(\"transcription.txt\", \"w\")\n",
    "\n",
    "    for blob in blob_list:\n",
    "        print(blob.name)\n",
    "\n",
    "    # Process the first output file from GCS.\n",
    "    # Since we specified batch_size=2, the first response contains\n",
    "    # the first two pages of the input file.\n",
    "    for n in  range(len(blob_list)):\n",
    "        output = blob_list[n]\n",
    "\n",
    "        json_string = output.download_as_string()\n",
    "        response = json.loads(json_string)\n",
    "\n",
    "\n",
    "        # The actual response for the first page of the input file.\n",
    "        for m in range(len(response['responses'])):\n",
    "\n",
    "            first_page_response = response['responses'][m]\n",
    "\n",
    "            try:\n",
    "                annotation = first_page_response['fullTextAnnotation']\n",
    "            except(KeyError):\n",
    "                print(\"No annotation for this page.\")\n",
    "\n",
    "            # Here we print the full text from the first page.\n",
    "            # The response contains more information:\n",
    "            # annotation/pages/blocks/paragraphs/words/symbols\n",
    "            # including confidence scores and bounding boxes\n",
    "            print('Full text:\\n')\n",
    "            print(annotation['text'])\n",
    "            \n",
    "            with open(\"transcription.txt\", \"a+\", encoding=\"utf-8\") as f:\n",
    "                f.write(annotation['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda3b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for the operation to finish.\n"
     ]
    }
   ],
   "source": [
    "async_detect_document(\"gs://ocr_bucket_one/LOTR/LOTR1.pdf\", \"gs://ocr_bucket_one/resultado-LOTR1-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d4846d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrite_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvision_bucket_ilicio/LOTR/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mwrite_to_text\u001b[0;34m(gcs_destination_uri)\u001b[0m\n\u001b[1;32m     40\u001b[0m storage_client \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m     42\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs://([^/]+)/(.+)\u001b[39m\u001b[38;5;124m'\u001b[39m, gcs_destination_uri)\n\u001b[0;32m---> 43\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m prefix \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     46\u001b[0m bucket \u001b[38;5;241m=\u001b[39m storage_client\u001b[38;5;241m.\u001b[39mget_bucket(bucket_name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "write_to_text(\"vision_bucket_ilicio/LOTR/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75cec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
